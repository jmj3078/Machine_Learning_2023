{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "--2023-03-16 17:54:03--  http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz\n",
      "Resolving nlp.cs.aueb.gr (nlp.cs.aueb.gr)... 195.251.248.252\n",
      "Connecting to nlp.cs.aueb.gr (nlp.cs.aueb.gr)|195.251.248.252|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1802573 (1.7M) [application/x-gzip]\n",
      "Saving to: 'data/enron1.tar.gz'\n",
      "\n",
      "enron1.tar.gz       100%[===================>]   1.72M   506KB/s    in 3.5s    \n",
      "\n",
      "2023-03-16 17:54:12 (506 KB/s) - 'data/enron1.tar.gz' saved [1802573/1802573]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc -P data http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf data/enron1.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks"
     ]
    }
   ],
   "source": [
    "!cat ./data/enron1/ham/0007.1999-12-14.farmer.ham.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "emails, labels = [], []\n",
    "partition = 0\n",
    "spam_file_path = './data/enron1/spam'\n",
    "ham_file_path = './data/enron1/ham'\n",
    "\n",
    "# email data 불러와서 활용하기\n",
    "\n",
    "for fname in glob.glob(os.path.join(spam_file_path, '*txt')):\n",
    "    with open(fname, 'r', encoding='ISO-8859-1') as f:\n",
    "        emails.append(f.read())\n",
    "        labels.append(1)\n",
    "\n",
    "for fname in glob.glob(os.path.join(ham_file_path, '*txt')):\n",
    "    with open(fname, 'r', encoding='ISO-8859-1') as f:\n",
    "        emails.append(f.read())\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: want to make more money ?\n",
      "order confirmation . your order should be shipped by january , via fedex .\n",
      "your federal express tracking number is % random _ word .\n",
      "thank you for registering . your userid is :\n",
      "% random _ word\n",
      "learn to make a fortune with ebay !\n",
      "complete turnkey system software - videos - turorials\n",
      "clck\n",
      "here if you would not like to receive future mailings .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(emails[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/mjcho/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mjcho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mjcho/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 - 자연어처리, nltk 패키지\n",
    "# 숫자와 구두점 표기 제거, 사람이름 제거, 불용어 제거, 표제어 원형 복원 등등\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "all_names = set(names.words())\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_only(word):\n",
    "    return word.isalpha()\n",
    "\n",
    "def clean_text(doc):\n",
    "    cleaned_doc = []\n",
    "    for word in doc.split(' '): # ' '기준으로 문장에서 단어 분리\n",
    "        word = word.lower() # 모두 소문자로 변경\n",
    "\n",
    "        if letters_only(word) and word not in all_names and len(word) > 2: \n",
    "            # 단어 길이가 2가 넘지 않는 경우, letters_only인 경우, all_in names에 포함되지 않은 경우 lemmatizer에 넣음\n",
    "            cleaned_doc.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return ' '.join(cleaned_doc) #문장으로 다시 만들어서 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_emails = [clean_text(doc) for doc in emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what your cam are you looking for your looking for companion for friendship love date just good ole then try our brand new site wa developed and help anyone find what they looking for quick bio form and you the road satisfaction every sense the word matter may out and youll amazed terrific time this and ste the add res you see the line below into your browser come the site www meganbang biz bld acc more www naturalgolden com retract aitken step preemptive shoehorn scaup electrocardiograph movie honeycomb monster war brandywine pietism byrne catatonia encomium lookup intervenor skeleton turn catfish'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, Training dataset split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized data 형태로의 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words='english', max_features=500)\n",
    "term_docs_train = cv.fit_transform(X_train) # Fit_transform을 사용해 feature로 사용 가능한 형태로 만든다\n",
    "term_docs_test = cv.transform(X_test) # Test data에 대해서는 transform을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 133)\t2\n",
      "  (0, 197)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 432)\t3\n"
     ]
    }
   ],
   "source": [
    "print(term_docs_train[0]) # frequency로 변환하여 벡터화를 진행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 121)\t1\n",
      "  (0, 133)\t1\n",
      "  (0, 257)\t1\n",
      "  (0, 395)\t1\n",
      "  (0, 409)\t1\n",
      "  (0, 432)\t1\n"
     ]
    }
   ],
   "source": [
    "print(term_docs_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enron\n",
      "actuals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "print(feature_names[133]) \n",
    "print(feature_names[9])\n",
    "#이런식으로 feature가 어떤 단어에 해당하는지 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "# 라플라스 smoothing factor를 1.0으로 결정하고, fit_prior는 True면 학습데이터 분포로 Prior설정\n",
    "clf.fit(term_docs_train, Y_train)\n",
    "# 모델 학습 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999999e-001, 1.12649655e-009],\n",
       "       [1.00000000e+000, 1.95684343e-028],\n",
       "       [1.00000000e+000, 8.30276362e-016],\n",
       "       [7.76762132e-011, 1.00000000e+000],\n",
       "       [1.00000000e+000, 3.67113996e-053],\n",
       "       [1.00000000e+000, 3.71256966e-143],\n",
       "       [1.00000000e+000, 7.76210164e-120],\n",
       "       [1.00000000e+000, 2.80767073e-016],\n",
       "       [1.00000000e+000, 7.03238079e-045],\n",
       "       [6.77193629e-001, 3.22806371e-001]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob = clf.predict_proba(term_docs_test)\n",
    "predict_prob[0:10] \n",
    "# 0(정상)에 대한 확률, 1(스팸)에 대한 확률 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using MultinomialNB is : 0.8984%\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(term_docs_test, Y_test)\n",
    "print(f'The accuracy using MultinomialNB is : {accuracy:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      1222\n",
      "           1       0.81      0.84      0.83       490\n",
      "\n",
      "    accuracy                           0.90      1712\n",
      "   macro avg       0.87      0.88      0.88      1712\n",
      "weighted avg       0.90      0.90      0.90      1712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(Y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
